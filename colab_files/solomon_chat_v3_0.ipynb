{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNYIMZgriiEYSKTH+blgjm6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/acdc-digital/acdc.cooksite/blob/master/colab_files/solomon_chat_v3_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### a. import dependencies & statements"
      ],
      "metadata": {
        "id": "rnvFix4vz0pi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1mo1KxJ8zvAW"
      },
      "outputs": [],
      "source": [
        "! pip install \"deeplake[enterprise]\"\n",
        "! pip install llama-index\n",
        "! pip install langchain\n",
        "! pip install nltk\n",
        "! pip install openai\n",
        "! pip install pandas\n",
        "! pip install pdfminer\n",
        "! pip install pdfminer.six\n",
        "! pip install plotly\n",
        "! pip install -U scikit-learn\n",
        "! pip install torch\n",
        "! pip install transformers\n",
        "! pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MASTER-CODEBLOCK\n",
        "##################################\n",
        "\n",
        "import os\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import numpy as np\n",
        "import openai\n",
        "import pandas as pd\n",
        "import re\n",
        "import torch\n",
        "from collections import Counter\n",
        "from deeplake.core.vectorstore import VectorStore\n",
        "from langchain.chains import RetrievalQA, ConversationalRetrievalChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.memory import ConversationTokenBufferMemory\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import DeepLake\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "from openai.embeddings_utils import get_embedding, cosine_similarity\n",
        "import pandas as pd\n",
        "from pdfminer.high_level import extract_text, extract_pages\n",
        "from sklearn.cluster import KMeans\n",
        "from tqdm import tqdm\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2TokenizerFast"
      ],
      "metadata": {
        "id": "fbxHcVlHz0EE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download necessary NLTK data\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Set the max cell size for text (32,767 characters = true limit)\n",
        "MAX_CELL_SIZE = 11250\n",
        "\n",
        "# Directory containing your PDFs\n",
        "pdf_directory = '/content/source_docs'\n",
        "\n",
        "# List to store data\n",
        "data = []\n",
        "\n",
        "# Wrap the loop with tqdm for a progress bar\n",
        "for pdf_file in tqdm(os.listdir(pdf_directory)):\n",
        "    if pdf_file.endswith('.pdf'):\n",
        "        file_path = os.path.join(pdf_directory, pdf_file)\n",
        "        try:\n",
        "            print(f\"Processing {pdf_file}...\")\n",
        "            text = extract_text(file_path)\n",
        "\n",
        "            if not text:\n",
        "                print(f\"Extracted text is empty for {pdf_file}\")\n",
        "                continue\n",
        "\n",
        "            text_words_set = set(text.lower().split())\n",
        "            filtered_words_set = text_words_set - stop_words\n",
        "            filtered_text = ' '.join(filtered_words_set)\n",
        "\n",
        "            # Basic heuristic: Assuming title is the first line and summary is the second line\n",
        "            lines = text.split('\\n')\n",
        "            title = lines[0] if len(lines) > 0 else ''\n",
        "            summary = lines[1] if len(lines) > 1 else ''\n",
        "\n",
        "            # Metadata extraction\n",
        "            file_size = os.path.getsize(file_path)\n",
        "            number_of_pages = len(list(extract_pages(file_path)))\n",
        "\n",
        "            # Filter Stopwords\n",
        "            text_words = text.split()\n",
        "            filtered_words = [word for word in text_words if word.lower() not in stop_words]\n",
        "            filtered_text = ' '.join(filtered_words)\n",
        "\n",
        "            # Text normalization\n",
        "            text = text.lower()\n",
        "\n",
        "            # Chunking the content\n",
        "            chunks = [filtered_text[i:i+MAX_CELL_SIZE] for i in range(0, len(filtered_text), MAX_CELL_SIZE)]\n",
        "            for chunk in chunks:\n",
        "                data.append({\n",
        "                    'filename': pdf_file,\n",
        "                    'title_or_heading': title,\n",
        "                    'content_summary': summary,\n",
        "                    'content_chunk': chunk,\n",
        "                    'file_size': file_size,\n",
        "                    'number_of_pages': number_of_pages\n",
        "                })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {pdf_file}: {e}\")\n",
        "            continue\n",
        "\n",
        "# Convert the list to a DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Save the DataFrame to a CSV for further analysis with escapechar\n",
        "df.to_csv('/content/source_csv/source_docs.csv', index=False, escapechar='\\\\')\n",
        "\n",
        "## print(\"Listing directory contents:\")\n",
        "## print(os.listdir(pdf_directory))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LcEpuAx00wSy",
        "outputId": "f3dd75cf-8909-464a-8acd-014fb1976fa2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing Contrast-Context-Scaling.pdf...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 2/5 [00:04<00:07,  2.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing Concepts-all-Need.pdf...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 3/5 [00:06<00:04,  2.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing Prompt-You-Need.pdf...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 4/5 [00:12<00:03,  3.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing AI-Evolution-Education.pdf...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5/5 [00:19<00:00,  3.94s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "generate the jsonl file, and store the output in the source_csv directory"
      ],
      "metadata": {
        "id": "-v9vOKYM5LXK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import os  # Import the os module\n",
        "\n",
        "# Define the directory where you want to save the .jsonl file\n",
        "save_directory = '/content/source_csv/'\n",
        "\n",
        "# Read the CSV file into a DataFrame\n",
        "df = pd.read_csv(os.path.join(save_directory, 'source_docs.csv'), escapechar='\\\\')\n",
        "\n",
        "# Extract the 'content_chunk' column\n",
        "content_chunks = df['content_chunk']\n",
        "\n",
        "# Tokenize the text (you can use a more advanced tokenizer if you wish)\n",
        "tokenized_chunks = [chunk.split() for chunk in content_chunks]\n",
        "\n",
        "# Open a JSONL file to write the JSON objects\n",
        "with open(os.path.join(save_directory, 'embeddings.jsonl'), 'w') as jsonl_file:\n",
        "    for idx, tokens in enumerate(tokenized_chunks):\n",
        "        # Create JSON object and write to JSONL file\n",
        "        json_obj = {\n",
        "            \"model\": \"text-embedding-ada-002\",\n",
        "            \"input\": tokens,\n",
        "            \"metadata\": {\"row_id\": idx}\n",
        "        }\n",
        "        jsonl_file.write(json.dumps(json_obj) + '\\n')"
      ],
      "metadata": {
        "id": "hwsN80_e4JPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import subprocess\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = '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'\n",
        "\n",
        "completed_process = subprocess.run([\n",
        "    \"python\", \"/content/parallel_processor/api_request_parallel_processor.py\",\n",
        "    \"--requests_filepath\", \"/content/source_csv/embeddings.jsonl\",\n",
        "    \"--save_filepath\", \"/content/parallel_processor/output_embeddings.jsonl\",\n",
        "    \"--request_url\", \"https://api.openai.com/v1/embeddings\",\n",
        "    \"--max_requests_per_minute\", \"1500\",\n",
        "    \"--max_tokens_per_minute\", \"6250000\",\n",
        "    \"--token_encoding_name\", \"cl100k_base\",\n",
        "    \"--max_attempts\", \"5\",\n",
        "    \"--logging_level\", \"20\"\n",
        "], capture_output=True, text=True)\n",
        "\n",
        "print(\"Return code:\", completed_process.returncode)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKSzOOIq4KM7",
        "outputId": "a8638cd7-204c-4648-de14-109f63c60efc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Return code: 0\n",
            "Have 0 bytes in stdout:\n",
            "\n",
            "Have 843 bytes in stderr:\n",
            "INFO:root:Starting request #0\n",
            "INFO:root:Starting request #1\n",
            "INFO:root:Starting request #2\n",
            "INFO:root:Starting request #3\n",
            "INFO:root:Starting request #4\n",
            "INFO:root:Starting request #5\n",
            "INFO:root:Starting request #6\n",
            "INFO:root:Starting request #7\n",
            "INFO:root:Starting request #8\n",
            "INFO:root:Starting request #9\n",
            "INFO:root:Starting request #10\n",
            "INFO:root:Starting request #11\n",
            "INFO:root:Starting request #12\n",
            "INFO:root:Starting request #13\n",
            "INFO:root:Starting request #14\n",
            "INFO:root:Starting request #15\n",
            "INFO:root:Starting request #16\n",
            "INFO:root:Starting request #17\n",
            "INFO:root:Starting request #18\n",
            "INFO:root:Starting request #19\n",
            "INFO:root:Starting request #20\n",
            "INFO:root:Starting request #21\n",
            "INFO:root:Starting request #22\n",
            "INFO:root:Starting request #23\n",
            "INFO:root:Parallel processing complete. Results saved to /content/parallel_processor/output_embeddings.jsonl\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('/content/parallel_processor/output_embeddings.jsonl')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Z3_Tb2Qx9PfZ",
        "outputId": "853da962-973d-46cb-be0e-440049076a2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_26f1d5e3-fb1f-4521-9c5d-3fd545987301\", \"output_embeddings.jsonl\", 667303252)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "# Initialize an empty list to store the JSON objects\n",
        "json_list = []\n",
        "\n",
        "# Read the JSONL file line by line\n",
        "with open('/content/parallel_processor/output_embeddings.jsonl', 'r') as f:\n",
        "    for line in f:\n",
        "        json_obj = json.loads(line.strip())\n",
        "        json_list.append(json_obj)\n",
        "\n",
        "# Convert the list of JSON objects to a DataFrame\n",
        "df_embeddings = pd.json_normalize(json_list)\n",
        "\n",
        "# Extract the actual numerical embeddings and row_ids\n",
        "df_embeddings['actual_embedding'] = df_embeddings.apply(lambda row: row[1]['data'][0]['embedding'], axis=1)\n",
        "df_embeddings['row_id'] = df_embeddings.apply(lambda row: row[2]['row_id'], axis=1)\n",
        "\n",
        "# Drop unnecessary columns\n",
        "df_embeddings = df_embeddings[['row_id', 'actual_embedding']]\n",
        "\n",
        "# DF is your existing DataFrame\n",
        "df = pd.read_csv('/content/source_csv/source_docs.csv')\n",
        "\n",
        "# Merge the existing DataFrame with the embeddings DataFrame based on row_id\n",
        "df_merged = pd.merge(df, df_embeddings, left_index=True, right_on='row_id', how='left')\n",
        "\n",
        "print(df_merged.head())\n",
        "\n",
        "# Save the merged DataFrame to a new CSV file\n",
        "df_merged.to_csv('/content/source_ada/source_ada.csv', index=False)\n",
        "print(df_merged.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-xciShHElsF",
        "outputId": "3f137748-eb44-4195-e230-51b460cc162b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                        filename title_or_heading content_summary  \\\n",
            "3   Contrast-Context-Scaling.pdf                3               2   \n",
            "11  Contrast-Context-Scaling.pdf                3               2   \n",
            "13  Contrast-Context-Scaling.pdf                3               2   \n",
            "2   Contrast-Context-Scaling.pdf                3               2   \n",
            "20  Contrast-Context-Scaling.pdf                3               2   \n",
            "\n",
            "                                        content_chunk  file_size  \\\n",
            "3   3 2 0 2 l u J 6 ] L C . c [ 1 v 0 7 1 3 0 . 7 ...     819956   \n",
            "11  level data loading pipeline minor self-attenti...     819956   \n",
            "13  text length. FOT demonstrates high accuracy ev...     819956   \n",
            "2   rplexity PG19 References Joshua Ainslie, Tao L...     819956   \n",
            "20  Katherine Lee, Sharan Narang, Michael Matena, ...     819956   \n",
            "\n",
            "    number_of_pages  row_id                                   actual_embedding  \n",
            "3                27       0  [0.00064371835, 0.00905348, 0.004182098, -0.02...  \n",
            "11               27       1  [0.029044466, -0.010953795, 0.015143432, -0.01...  \n",
            "13               27       2  [-0.009887135, -0.015208861, 0.008635772, -0.0...  \n",
            "2                27       3  [0.016047653, -0.0012377744, -0.006450249, -0....  \n",
            "20               27       4  [-0.014680965, 0.005562084, 0.012885883, -0.02...  \n",
            "Index(['filename', 'title_or_heading', 'content_summary', 'content_chunk',\n",
            "       'file_size', 'number_of_pages', 'row_id', 'actual_embedding'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MASTER-CODEBLOCK\n",
        "##################################\n",
        "\n",
        "os.environ['ACTIVELOOP_TOKEN'] = 'eyJhbGciOiJIUzUxMiIsImlhdCI6MTY5MDIwMDcxNCwiZXhwIjoxNzA0MDI4MjU5fQ.eyJpZCI6ImFjZGNkaWdpdGFsIn0.RwLAU6QDB2GrMGyu2XImbHajwsEpb6PMDe_IGQ8pzE4tEKCQHXUZCAdry4f9KUtt2eHktNpxBq7XI6AkDA9Mnw'\n",
        "# Load DataFrame from CSV\n",
        "df = pd.read_csv('/content/source_ada/source_ada.csv')\n",
        "\n",
        "# Prepare data\n",
        "chunked_text = df['content_chunk'].tolist()\n",
        "source_texts = df['filename'].tolist()\n",
        "precomputed_embeddings = df['actual_embedding'].apply(eval).tolist()  # Assuming embeddings are stored as strings\n",
        "\n",
        "# Initialize Vector Store with the Hub URL\n",
        "vector_store_path = \"hub://solomon/solov3-enginetest1\"\n",
        "vector_store = VectorStore(\n",
        "    path=vector_store_path,\n",
        ")\n",
        "\n",
        "# Add data to Vector Store\n",
        "vector_store.add(\n",
        "    text=chunked_text,\n",
        "    embedding=precomputed_embeddings,\n",
        "    metadata=[{\"source\": source_text} for source_text in source_texts]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfa7oKe3EmY8",
        "outputId": "d593258d-e482-4ef3-c547-d8fb839a4e15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your Deep Lake dataset has been successfully created!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 24/24 [00:01<00:00, 23.71it/s]\n",
            "/"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset(path='hub://solomon/solov3-enginetest1', tensors=['text', 'metadata', 'embedding', 'id'])\n",
            "\n",
            "  tensor      htype      shape      dtype  compression\n",
            "  -------    -------    -------    -------  ------- \n",
            "   text       text      (24, 1)      str     None   \n",
            " metadata     json      (24, 1)      str     None   \n",
            " embedding  embedding  (24, 1536)  float32   None   \n",
            "    id        text      (24, 1)      str     None   \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r \r"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(os.environ.get('OPENAI_API_KEY'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16iEkraLJW1I",
        "outputId": "48edc51f-e07f-4118-e759-f38152228b87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "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\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MASTER-CODEBLOCK\n",
        "##################################\n",
        "\n",
        "# Initialize OpenAI\n",
        "import os\n",
        "import openai\n",
        "import openai\n",
        "openai.api_key = \"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\"\n",
        "\n",
        "# Your embedding function\n",
        "def embedding_function(texts, model=\"text-embedding-ada-002\"):\n",
        "    if isinstance(texts, str):\n",
        "        texts = [texts]\n",
        "    texts = [t.replace(\"\\n\", \" \") for t in texts]\n",
        "    return [data['embedding'] for data in openai.Embedding.create(input=texts, model=model)['data']]\n",
        "\n",
        "# Wrap your function in a class with an embed_query method\n",
        "class MyEmbeddingFunction:\n",
        "    def __init__(self, func):\n",
        "        self.func = func\n",
        "\n",
        "    def embed_query(self, query):\n",
        "        return self.func(query)\n",
        "\n",
        "# Initialize DeepLake database with the embedding_function\n",
        "embedding_function_obj = MyEmbeddingFunction(embedding_function)\n",
        "db = DeepLake(dataset_path=\"hub://solomon/solov3-enginetest1\", embedding=embedding_function_obj, read_only=False)\n",
        "\n",
        "# Initialize Retriever with parameters\n",
        "retriever = db.as_retriever()\n",
        "retriever.search_kwargs.update({\n",
        "    'distance_metric': 'cos',\n",
        "    'k': 1\n",
        "})\n",
        "\n",
        "# Define the PromptTemplate\n",
        "template = \"\"\"You are Solomon, a specialized personal assistant. Your expertise spans all areas of life, including technical documents and complex arguments. Use the following pieces of retrieved context to answer any questions that come up. If you don't know the answer, just say that you don't know.\n",
        "{context}\n",
        "Question: {question}\n",
        "Helpful Answer:\n",
        "\"\"\"\n",
        "\n",
        "# Create a PromptTemplate object\n",
        "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
        "\n",
        "# Initialize LLM for QA\n",
        "model = ChatOpenAI(model='gpt-4')\n",
        "\n",
        "# Initialize Langchain Memory with Token Buffer\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.langchain.plus\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = \"ls__fbfe7701decf42138ac5d036eb60afc5\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"solomon.v2.2\"\n",
        "\n",
        "memory = ConversationTokenBufferMemory(  # <-- Changed to ConversationTokenBufferMemory\n",
        "    llm=model,\n",
        "    max_token_limit=450,\n",
        "    memory_key=\"chat_history\",\n",
        "    return_messages=True\n",
        ")\n",
        "\n",
        "# Initialize Conversational Retrieval Chain with Memory\n",
        "qa = ConversationalRetrievalChain.from_llm(\n",
        "    llm=model,\n",
        "    retriever=retriever,\n",
        "    memory=memory\n",
        ")\n",
        "\n",
        "# Define your search query\n",
        "search_query = 'How is AI being used in the evolution of education?'\n",
        "\n",
        "# Count the number of tokens in the search query and prompt\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
        "search_query_tokens = tokenizer.encode(search_query, truncation=True)\n",
        "prompt_tokens = tokenizer.encode(template, truncation=True)\n",
        "num_search_query_tokens = len(search_query_tokens)\n",
        "num_prompt_tokens = len(prompt_tokens)\n",
        "\n",
        "# Run the QA model with top-k documents\n",
        "result = qa({\"question\": search_query})\n",
        "response = result['answer']\n",
        "print(\"QA Response:\")\n",
        "print(response)\n",
        "\n",
        "# Count the number of tokens in the generated response\n",
        "response_tokens = tokenizer.encode(response, truncation=True)\n",
        "num_response_tokens = len(response_tokens)\n",
        "\n",
        "# Print token counts\n",
        "print(f\"Number of tokens in the search query: {num_search_query_tokens}\")\n",
        "print(f\"Number of tokens in the prompt: {num_prompt_tokens}\")\n",
        "print(f\"Number of tokens in the generated response: {num_response_tokens}\")\n",
        "\n",
        "# Extract and print unique sources (Top 3)\n",
        "print(\"\\nUnique Sources:\")\n",
        "docs = retriever.get_relevant_documents(search_query)\n",
        "unique_sources = set(doc.metadata.get('source', 'N/A') for doc in docs)\n",
        "unique_sources_top3 = list(unique_sources)[:3]\n",
        "print(unique_sources_top3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhA3brK0Irxh",
        "outputId": "009f1422-7768-4376-f34c-509f51537554"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deep Lake Dataset in hub://solomon/solov3-enginetest1 already exists, loading from the storage\n",
            "QA Response:\n",
            "I'm sorry, but the provided information does not contain details on how AI is being used in the evolution of education.\n",
            "Number of tokens in the search query: 11\n",
            "Number of tokens in the prompt: 71\n",
            "Number of tokens in the generated response: 24\n",
            "\n",
            "Unique Sources:\n",
            "['Prompt-You-Need.pdf']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "spk39g90KCqw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}